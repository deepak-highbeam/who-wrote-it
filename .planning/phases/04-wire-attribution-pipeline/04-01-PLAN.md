---
phase: 04-wire-attribution-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/store/sqlite.go
  - internal/daemon/daemon.go
  - internal/daemon/daemon_test.go
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Daemon processes file events into attribution records automatically as events arrive"
    - "Each attribution has authorship level, confidence, and work-type assigned"
    - "who-wrote-it analyze produces non-empty reports when daemon has been running"
    - "Attribution pipeline runs periodically without blocking other daemon subsystems"
  artifacts:
    - path: "internal/store/sqlite.go"
      provides: "QueryUnprocessedFileEvents method"
      contains: "QueryUnprocessedFileEvents"
    - path: "internal/daemon/daemon.go"
      provides: "Background attribution processor goroutine"
      contains: "startAttributionProcessor"
    - path: "internal/daemon/daemon_test.go"
      provides: "Integration test for attribution pipeline"
      contains: "TestAttributionPipeline"
  key_links:
    - from: "internal/daemon/daemon.go"
      to: "internal/correlation/correlator.go"
      via: "correlator.CorrelateFileEvent(fe)"
      pattern: "correlation\\.New"
    - from: "internal/daemon/daemon.go"
      to: "internal/authorship/classifier.go"
      via: "classifier.Classify(result)"
      pattern: "authorship\\.NewClassifier"
    - from: "internal/daemon/daemon.go"
      to: "internal/worktype/classifier.go"
      via: "wtClassifier.ClassifyFile(filePath, diff, commit)"
      pattern: "worktype\\.NewClassifier"
    - from: "internal/daemon/daemon.go"
      to: "internal/store/sqlite.go"
      via: "store.InsertAttribution(attr)"
      pattern: "InsertAttribution"
---

<objective>
Wire the existing correlation engine, authorship classifier, and work-type classifier into the daemon's event processing lifecycle via a background processor goroutine.

Purpose: Closes the critical integration gap identified in the v1 milestone audit. All three intelligence packages (correlation, authorship, worktype) exist and are tested, but the daemon never invokes them. File events go into the store but are never processed into attributions, breaking 4 of 5 E2E user flows.

Output: A working attribution pipeline that runs inside the daemon, processing file events into fully-classified attribution records with authorship levels and work types stored in SQLite.
</objective>

<execution_context>
@/Users/deepakbhimaraju/.claude/get-shit-done/workflows/execute-plan.md
@/Users/deepakbhimaraju/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/v1-MILESTONE-AUDIT.md
@internal/daemon/daemon.go
@internal/correlation/correlator.go
@internal/authorship/classifier.go
@internal/worktype/classifier.go
@internal/store/sqlite.go
@internal/store/schema.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add QueryUnprocessedFileEvents store method and background attribution processor</name>
  <files>internal/store/sqlite.go, internal/daemon/daemon.go</files>
  <action>
  **Step 1: Add QueryUnprocessedFileEvents to store (internal/store/sqlite.go)**

  Add a new method on `*Store` that returns file events which have no corresponding attribution record yet:

  ```go
  // QueryUnprocessedFileEvents returns file events that have no matching attribution record.
  // Uses a LEFT JOIN against attributions on file_event_id to find unprocessed events.
  // Ordered by timestamp ascending so oldest events are processed first.
  // Limits to batchSize rows per call to bound processing time.
  func (s *Store) QueryUnprocessedFileEvents(batchSize int) ([]FileEvent, error) {
  ```

  SQL: `SELECT fe.id, fe.project_path, fe.file_path, fe.event_type, fe.timestamp FROM file_events fe LEFT JOIN attributions a ON a.file_event_id = fe.id WHERE a.id IS NULL ORDER BY fe.timestamp ASC LIMIT ?`

  Reuse `scanFileEvents` for row scanning. Parameter `batchSize` defaults to 100 if <= 0.

  **Step 2: Add background attribution processor to daemon (internal/daemon/daemon.go)**

  Add three new imports:
  - `"github.com/anthropic/who-wrote-it/internal/authorship"`
  - `"github.com/anthropic/who-wrote-it/internal/correlation"`
  - `"github.com/anthropic/who-wrote-it/internal/worktype"`

  Add a new field to the Daemon struct:
  - `attrCancel context.CancelFunc` for the attribution processor goroutine

  Add a new method `startAttributionProcessor(ctx context.Context)` that:
  1. Creates a `correlation.New(d.store)` correlator
  2. Creates an `authorship.NewClassifier()` classifier
  3. Creates a `worktype.NewClassifier(d.store)` work-type classifier (store implements OverrideReader)
  4. Runs a ticker loop (every 2 seconds) that:
     a. Calls `d.store.QueryUnprocessedFileEvents(100)` to get a batch of unprocessed events
     b. For each file event:
        - Calls `correlator.CorrelateFileEvent(fe)` to get a `*authorship.CorrelationResult`
        - Calls `classifier.Classify(*result)` to get an `authorship.Attribution`
        - Calls `wtClassifier.ClassifyFile(attr.FilePath, "", "")` to get a `worktype.WorkType` (empty diff and commit for now -- diff content not stored in file_events)
        - Converts Attribution to `store.AttributionRecord`:
          - FilePath, ProjectPath from the Attribution
          - FileEventID, SessionEventID from the Attribution (pointer fields)
          - AuthorshipLevel = string(attr.Level)
          - Confidence = attr.Confidence
          - Uncertain = attr.Uncertain
          - FirstAuthor = attr.FirstAuthor
          - CorrelationWindowMs = attr.CorrelationWindowMs
          - Timestamp = attr.Timestamp
        - Calls `d.store.InsertAttribution(record)` to persist
        - If InsertAttribution returns an ID, calls `d.store.UpdateAttributionWorkType(id, string(wt))` to set work type
     c. Logs processing count: `log.Printf("attribution: processed %d events", len(events))`
        Only log when len(events) > 0 to avoid spam.
  5. On context cancellation, returns cleanly.

  Wire into daemon lifecycle:
  - In `Start()`, after the git integration block and before the final log line, add:
    ```go
    attrCtx, attrCancel := context.WithCancel(d.ctx)
    d.attrCancel = attrCancel
    d.startAttributionProcessor(attrCtx)
    ```
  - In `shutdown()`, cancel the attribution processor AFTER session and git cancels (it needs those to have flushed), but BEFORE store close:
    ```go
    if d.attrCancel != nil {
        d.attrCancel()
    }
    ```
    Place this after the gitCancel block and before the watcher stop.

  **Important design decisions to follow:**
  - 2-second polling interval (balances latency with CPU -- events typically arrive in bursts during active development)
  - Batch size of 100 (bounds per-tick processing time)
  - Empty diff/commit for work-type classification (file_events don't store diff content; path-based heuristics still work for test files, config files, architecture files)
  - Non-fatal errors: log and continue processing other events (consistent with existing daemon pattern)
  - No special handling needed for the `correlation.StoreReader` interface -- `*store.Store` already satisfies it via QueryFileEventsInWindow, QueryFileEventsByProject, QuerySessionEventsInWindow, QuerySessionEventsNearTimestamp
  - No special handling needed for the `worktype.OverrideReader` interface -- `*store.Store` already satisfies it via QueryWorkTypeOverride
  </action>
  <verify>
  ```bash
  cd /path/to/who-wrote-it
  go build ./...     # Zero errors
  go vet ./...       # Zero warnings
  grep -n "correlation\|authorship\|worktype" internal/daemon/daemon.go  # All three packages imported and used
  grep -n "startAttributionProcessor" internal/daemon/daemon.go          # Method defined and called
  grep -n "QueryUnprocessedFileEvents" internal/store/sqlite.go          # New store method exists
  grep -n "InsertAttribution" internal/daemon/daemon.go                  # Attribution insertion happens in daemon
  ```
  </verify>
  <done>
  daemon.go imports correlation, authorship, and worktype packages. startAttributionProcessor method exists and is called in Start(). shutdown() cancels the attribution goroutine. QueryUnprocessedFileEvents returns file events with no matching attribution. go build and go vet pass with zero errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add integration test for attribution pipeline</name>
  <files>internal/daemon/daemon_test.go</files>
  <action>
  Create `internal/daemon/daemon_test.go` with an integration test that verifies the full attribution pipeline without starting the real daemon.

  Test: `TestAttributionPipeline`

  This test directly exercises the same code path the daemon uses:
  1. Create a temp directory and open a `store.New(tempDir + "/test.db")` store.
  2. Insert a file event: `store.InsertFileEvent("/testproject", "/testproject/main.go", "write", time.Now())`
  3. Insert a session event: `store.InsertSessionEvent("sess1", "tool_use", "Write", "/testproject/main.go", "abc123", time.Now(), "{}")` -- same file, same timestamp, should correlate as exact_file match.
  4. Create correlator: `correlation.New(store)`
  5. Create classifier: `authorship.NewClassifier()`
  6. Create worktype classifier: `worktype.NewClassifier(store)`
  7. Call `store.QueryUnprocessedFileEvents(100)` -- assert returns 1 event.
  8. For the returned event, run the full pipeline:
     - `correlator.CorrelateFileEvent(fe)` -- assert non-nil result, MatchType == "exact_file"
     - `classifier.Classify(*result)` -- assert Level is FullyAI (exact match < 2s)
     - `wtClassifier.ClassifyFile(fe.FilePath, "", "")` -- assert returns CoreLogic (default for .go files)
     - Build AttributionRecord and call `store.InsertAttribution(record)` -- assert no error, returns ID > 0
     - Call `store.UpdateAttributionWorkType(id, string(wt))` -- assert no error
  9. Verify: call `store.QueryUnprocessedFileEvents(100)` again -- assert returns 0 events (already processed).
  10. Verify: call `store.QueryAttributionsByProject("/testproject")` -- assert returns 1 record with:
     - AuthorshipLevel == "fully_ai"
     - FirstAuthor == "ai"
     - Confidence == 0.95
     - ProjectPath == "/testproject"

  Also add `TestAttributionPipelineNoSessionMatch`:
  1. Insert a file event only (no session event).
  2. Run the pipeline.
  3. Assert Level == FullyHuman, Confidence == 1.0, FirstAuthor == "human".

  Use `t.TempDir()` for temp directory. Import only standard library + project packages (no test frameworks).
  </action>
  <verify>
  ```bash
  cd /path/to/who-wrote-it
  go test -race -v ./internal/daemon/ -run TestAttribution
  ```
  Both tests pass with race detector enabled.
  </verify>
  <done>
  Two integration tests pass: TestAttributionPipeline verifies file events are correlated, classified (fully_ai), work-typed (core_logic), and stored as attributions. TestAttributionPipelineNoSessionMatch verifies unmatched events produce fully_human attributions. Both pass with -race flag.
  </done>
</task>

</tasks>

<verification>
Full verification of Phase 4 gap closure:

```bash
# 1. All code compiles
go build ./...

# 2. All tests pass (including new + existing)
go test -race ./...

# 3. Daemon imports all three intelligence packages
grep -c "correlation\|authorship\|worktype" internal/daemon/daemon.go

# 4. Attribution pipeline is wired
grep "startAttributionProcessor" internal/daemon/daemon.go
grep "InsertAttribution" internal/daemon/daemon.go
grep "CorrelateFileEvent" internal/daemon/daemon.go
grep "Classify" internal/daemon/daemon.go
grep "ClassifyFile" internal/daemon/daemon.go

# 5. Store method for unprocessed events exists
grep "QueryUnprocessedFileEvents" internal/store/sqlite.go

# 6. Integration tests exercise full pipeline
go test -v ./internal/daemon/ -run TestAttribution
```
</verification>

<success_criteria>
1. `go build ./...` passes with zero errors
2. `go test -race ./...` passes all existing and new tests
3. daemon.go imports and uses correlation, authorship, and worktype packages
4. startAttributionProcessor goroutine runs in daemon lifecycle (started in Start, cancelled in shutdown)
5. QueryUnprocessedFileEvents store method returns file events without attribution records
6. Integration tests verify the complete pipeline: file event -> correlation -> classify -> work-type -> InsertAttribution
7. The gap from the v1 milestone audit is closed: all 8 previously unsatisfied requirements (CORR-01, CORR-02, AUTH-01, AUTH-02, WTYP-01, WTYP-02, METR-01, METR-02) are now functionally satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/04-wire-attribution-pipeline/04-01-SUMMARY.md`
</output>
